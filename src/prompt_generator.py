import random

from typing import Any

camera_pose_list = [
    "left", "right", "leftward", "rightward",
    "up", "down", "upward", "downward",
    "forward", "backward", "clockwise", "counterclockwise",
]

camera_pose_explanation = {
    "left": "Leftward translation ‚Äì The camera moved leftward horizontally.",
    "right": "Rightward translation ‚Äì The camera moved rightward horizontally.",
    "leftward": "Leftward rotation ‚Äì The camera rotated leftward horizontally.",
    "rightward": "Rightward rotation ‚Äì The camera rotated rightward horizontally.",
    "up": "Upward translation ‚Äì The camera moved upward vertically.",
    "down": "Downward translation ‚Äì The camera moved downward vertically.",
    "upward": "Upward rotation ‚Äì The camera rotated upward vertically.",
    "downward": "Downward rotation ‚Äì The camera rotated downward vertically.",
    "forward": "Forward translation ‚Äì The camera moved forward along the viewing direction.",
    "backward": "Backward translation ‚Äì The camera moved backward along the viewing direction.",
    "clockwise": "Clockwise rotation ‚Äì The camera rotated clockwise along its viewing axis.",
    "counterclockwise": "Counterclockwise rotation ‚Äì The camera rotated counterclockwise along its viewing axis.",

    "no movement": "No movement ‚Äì The camera did not change its position or orientation.",
}

single_dof_map = {
    "phi": {
        "options": ["leftward", "rightward"],
        "prompt": {
            "zero-shot": "",
            "dataset-prior-hint": """
            Further Explanation:
            To simplify this question, note that the given image pair is generated by moving the camera along only one of its six degrees of freedom (DoF). Therefore, you do not need to estimate the motion across all six DoFs. Instead, focus solely on identifying the most significant DoF in this scenario. For this task, the dominant motion is a **camera yaw rotation**, which means leftward or rightward rotation.
            """,
            "CoT-hint": """
            Chain-of-Thought (CoT) Reasoning Hint:
            Below are some cues to help you reason through this task step by step.

            - Case 1: The camera rotated leftward (yaw rotation to the left)
            If the objects in the target image appear shifted to the right relative to their positions in the source image, then the camera most likely rotated leftward.

            - Case 2: The camera rotated rightward (yaw rotation to the right)
            If the objects in the target image appear shifted to the left relative to their positions in the source image, then the camera most likely rotated rightward.
            """,
            "VoT-hint": """
            Vision-of-Thought (VoT) Reasoning Hint:
            Below is some **ASCII Art** to visually illustrate how to reason through this task.

            - Case 1: The camera rotated leftward (yaw rotation to the left)

            Example:
            --------         --------
            |  ‚≠ïÔ∏è   |  --->   |    ‚≠ïÔ∏è |
            --------         --------
            (source)         (target)

            Interpretation:
            If the object appears to shift to the right in the target image, the camera most likely rotated leftward.

            Camera motion diagram:
            ‚≠ïÔ∏è      leftward rotation      ‚≠ïÔ∏è
            |       ----------->         |
            üì∑                           üì∑

            - Case 2: The camera rotated rightward (yaw rotation to the right)

            Example:
            --------         --------
            |    ‚≠ïÔ∏è |  --->   | ‚≠ïÔ∏è    |
            --------         --------
            (source)         (target)

            Interpretation:
            If the object appears to shift to the left in the target image, the camera most likely rotated rightward.

            Camera motion diagram:
            ‚≠ïÔ∏è     rightward rotation     ‚≠ïÔ∏è
            |       ----------->            |
            üì∑                              üì∑
            """,
        }
    },
    "theta": {
        "options": ["upward", "downward"],
        "prompt": {
            "zero-shot": "",
            "dataset-prior-hint": """
            Further Explanation:
            To simplify this question, note that the given image pair is generated by moving the camera along only one of its six degrees of freedom (DoF). Therefore, you do not need to estimate the motion across all six DoFs. Instead, focus solely on identifying the most significant DoF in this scenario. For this task, the dominant motion is a **camera pitch rotation**, which means upward or downward rotation.
            """,
            "CoT-hint": """
            Chain-of-Thought (CoT) Reasoning Hint:
            Below are some cues to help you reason through this task step by step.

            - Case 1: The camera rotated upward (pitch rotation upward)
            If the objects in the target image appear shifted downward relative to their positions in the source image, then the camera most likely rotated upward.

            - Case 2: The camera rotated downward (pitch rotation downward)
            If the objects in the target image appear shifted upward relative to their positions in the source image, then the camera most likely rotated downward.
            """,
            "VoT-hint": """
            Vision-of-Thought (VoT) Reasoning Hint:
            Below is some **ASCII Art** to visually illustrate how to reason through this task.

            - Case 1: The camera rotated upward (pitch rotation upward)

            Example:
            --------         --------
            |  ‚≠ïÔ∏è   | ---->  |       |
            |       | ---->  |  ‚≠ïÔ∏è   |
            --------         --------
            (source)         (target)

            Interpretation:
            If the object appears to shift downward in the target image, the camera most likely rotated upward.

            - Case 2: The camera rotated downward (pitch rotation downward)

            Example:
            --------         --------
            |       | ---->  |   ‚≠ïÔ∏è  |
            |   ‚≠ïÔ∏è  | ---->  |       |
            --------         --------
            (source)         (target)

            Interpretation:
            If the object appears to shift upward in the target image, the camera most likely rotated downward.
            """,
        },
    },
    "psi": {
        "options": ["clockwise", "counterclockwise"],
        "prompt": {
            "zero-shot": "",
            "dataset-prior-hint": """
            Further Explanation:
            To simplify this question, note that the given image pair is generated by moving the camera along only one of its six degrees of freedom (DoF). Therefore, you do not need to estimate the motion across all six DoFs. Instead, focus solely on identifying the most significant DoF in this scenario. For this task, the dominant motion is a **camera roll rotation**, which means clockwise or counterclockwise rotation.
            """,
            "CoT-hint": """
            Chain-of-Thought (CoT) Reasoning Hint:
            Below are some cues to help you reason through this task step by step.

            - Case 1: The camera rotated clockwise (roll rotation clockwise)
            If the objects in the target image appear shifted counterclockwise relative to their positions in the source image, then the camera most likely rotated clockwise.

            - Case 2: The camera rotated counterclockwise (roll rotation counterclockwise)
            If the objects in the target image appear shifted clockwise relative to their positions in the source image, then the camera most likely rotated counterclockwise.
            """,
            "VoT-hint": """
            Vision-of-Thought (VoT) Reasoning Hint:
            Below is some **ASCII Art** to visually illustrate how to reason through this task.

            - Case 1: The camera rotated clockwise (roll rotation clockwise)

            Example:
            --------         --------
            |       | ----> |      ‚≠ïÔ∏è |
            |     ‚≠ïÔ∏è| ----> |        |
            |       | ----> |        |
            --------         --------
            (source)         (target)

            Interpretation:
            If the object appears to shift counterclockwise in the target image, the camera most likely rotated clockwise.

            - Case 2: The camera rotated counterclockwise (roll rotation counterclockwise)

            Example:
            --------         --------
            |       | ----> |        |
            |     ‚≠ïÔ∏è| ----> |        |
            |       | ----> |     ‚≠ïÔ∏è |
            --------         --------
            (source)         (target)

            Interpretation:
            If the object appears to shift clockwise in the target image, the camera most likely rotated counterclockwise.
            """,
        },
    },
    "tx": {
        "options": ["left", "right"],
        "prompt": {
            "zero-shot": "",
            "dataset-prior-hint": """
            Further Explanation:
            To simplify this question, note that the given image pair is generated by moving the camera along only one of its six degrees of freedom (DoF). Therefore, you do not need to estimate the motion across all six DoFs. Instead, focus solely on identifying the most significant DoF in this scenario. For this task, the dominant motion is a **camera translation**, which means leftward or rightward translation.
            """,
            "CoT-hint": """
            Chain-of-Thought (CoT) Reasoning Hint:
            Below are some cues to help you reason through this task step by step.

            - Case 1: The camera moved leftward (translation to the left)
            If the objects in the target image appear shifted to the right relative to their positions in the source image, then the camera most likely moved leftward.
            
            - Case 2: The camera moved rightward (translation to the right)
            If the objects in the target image appear shifted to the left relative to their positions in the source image, then the camera most likely moved rightward.
            """,
            "VoT-hint": """
            Vision-of-Thought (VoT) Reasoning Hint:
            Below is some **ASCII Art** to visually illustrate how to reason through this task.

            - Case 1: The camera moved leftward (translation to the left)

            Example:
            --------         --------
            |  ‚≠ïÔ∏è   |  --->   |    ‚≠ïÔ∏è |
            --------         --------
            (source)         (target)

            Interpretation:
            If the object appears to shift to the right in the target image, the camera most likely moved leftward.

            Camera motion diagram:
            ‚≠ïÔ∏è      leftward translation      ‚≠ïÔ∏è
            |       ----------->         |
            üì∑                           üì∑

            - Case 2: The camera moved rightward (translation to the right)

            Example:
            --------         --------
            |    ‚≠ïÔ∏è |  --->   | ‚≠ïÔ∏è    |
            --------         --------
            (source)         (target)

            Interpretation:
            If the object appears to shift to the left in the target image, the camera most likely moved rightward.

            Camera motion diagram:
            ‚≠ïÔ∏è     rightward translation     ‚≠ïÔ∏è
            |       ----------->                |
            üì∑                                  üì∑
            """,
        }
    },
    "ty": {
        "options": ["up", "down"],
        "prompt": {
            "zero-shot": "",
            "dataset-prior-hint": """
            Further Explanation:
            To simplify this question, note that the given image pair is generated by moving the camera along only one of its six degrees of freedom (DoF). Therefore, you do not need to estimate the motion across all six DoFs. Instead, focus solely on identifying the most significant DoF in this scenario. For this task, the dominant motion is a **camera translation**, which means upward or downward translation.
            """,
            "CoT-hint": """
            Chain-of-Thought (CoT) Reasoning Hint:
            Below are some cues to help you reason through this task step by step.

            - Case 1: The camera moved upward (translation upward)
            If the objects in the target image appear shifted downward relative to their positions in the source image, then the camera most likely moved upward.

            - Case 2: The camera moved downward (translation downward)
            If the objects in the target image appear shifted upward relative to their positions in the source image, then the camera most likely moved downward.
            """,
            "VoT-hint": """
            Vision-of-Thought (VoT) Reasoning Hint:
            Below is some **ASCII Art** to visually illustrate how to reason through this task.

            - Case 1: The camera moved upward (translation upward)

            Example:
            --------         --------
            |  ‚≠ïÔ∏è   | ---->  |       |
            |       | ---->  |  ‚≠ïÔ∏è   |
            --------         --------
            (source)         (target)

            Interpretation:
            If the object appears to shift downward in the target image, the camera most likely moved upward.

            - Case 2: The camera moved downward (translation downward)

            Example:
            --------         --------
            |       | ---->  |   ‚≠ïÔ∏è  |
            |   ‚≠ïÔ∏è  | ---->  |       |
            --------         --------
            (source)         (target)

            Interpretation:
            If the object appears to shift upward in the target image, the camera most likely moved downward.
            """,
        },
    },
    "tz": {
        "options": ["forward", "backward"],
        "prompt": {
            "zero-shot": "",
            "dataset-prior-hint": """
            Further Explanation:
            To simplify this question, note that the given image pair is generated by moving the camera along only one of its six degrees of freedom (DoF). Therefore, you do not need to estimate the motion across all six DoFs. Instead, focus solely on identifying the most significant DoF in this scenario. For this task, the dominant motion is a **camera translation**, which means forward or backward translation.
            """,
            "CoT-hint": """
            Chain-of-Thought (CoT) Reasoning Hint:
            Below are some cues to help you reason through this task step by step.

            - Case 1: The camera moved forward (translation forward)
            If the objects in the target image appear larger or closer relative to their positions in the source image, then the camera most likely moved forward.

            - Case 2: The camera moved backward (translation backward)
            If the objects in the target image appear smaller or farther away relative to their positions in the source image, then the camera most likely moved backward.
            """,
            "VoT-hint": """
            Vision-of-Thought (VoT) Reasoning Hint:
            Below is some **ASCII Art** to visually illustrate how to reason through this task.

            - Case 1: The camera moved forward (translation forward)

            Example:
            --------         -----
            |   ‚≠ïÔ∏è  | ---->  | ‚≠ïÔ∏è |
            --------         -----
            (source)         (target)

            Interpretation:
            If the object appears larger or closer in the target image, the camera most likely moved forward.

            - Case 2: The camera moved backward (translation backward)

            Example:
            -----         --------
            | ‚≠ïÔ∏è | ---->  |   ‚≠ïÔ∏è  |
            -----         --------
            (source)         (target)

            Interpretation:
            If the object appears smaller or farther away in the target image, the camera most likely moved backward.
            """,
        },
    }
}

obj_centered_map = {
    "translation": {
        "options": ["left", "right"],
        "prompt": {
            "task": """
            Further Explanation:
            Note that the given image pair is generated by moving the camera while centering on the same scene. To simplify this question, you do not need to estimate the motion across all six DoFs. Instead, focus solely on the **camera translation**, which means leftward or rightward translation.
            """,
        },
    },
    
    "rotation": {
        "options": ["leftward", "rightward"],
        "prompt": {
            "task": """
            Further Explanation:
            Note that the given image pair is generated by moving the camera while centering on the same scene. To simplify this question, you do not need to estimate the motion across all six DoFs. Instead, focus solely on the **camera rotation**, which means leftward or rightward rotation.
            """,
        },
    },
}

total_options_map = {
    "single-dof-cls": single_dof_map,
    "obj-centered-cls": obj_centered_map,
}

def _generate_options_text(options: list):
    text = "\n".join(f"{idx}. {camera_pose_explanation[f'{option}']}" for idx, option in enumerate(options))
    option_map = {idx: option for idx, option in enumerate(options)}
    return text, option_map


class PromptGenerator:
    def __init__(self, config: Any):
        self.config = config # global config

    def _postprocess_for_options(self, options: list) -> list:
        is_trap = self.config.STRATEGY.IS_TRAP
        if is_trap:
            options.append("no movement")

        is_shuffle = self.config.STRATEGY.IS_SHUFFLE
        if is_shuffle:
            random.shuffle(options)
        return options

    def _generate_prompt_strategy(self):
        self.config.EXPERIMENT.TASK_SPLIT
        self.config.STRATEGY.VLM_ONLY.PROMPT_TYPE

        prompt_strategy_map = total_options_map[self.config.EXPERIMENT.TASK_NAME][self.config.EXPERIMENT.TASK_SPLIT]["prompt"]
        
        if self.config.EXPERIMENT.TASK_NAME == "single-dof-cls":
            if self.config.STRATEGY.VLM_ONLY.PROMPT_TYPE in ["CoT-hint", "VoT-hint"]:
                prompt = prompt_strategy_map["dataset-prior-hint"] # concat dataset prior hint first.
                prompt += prompt_strategy_map[self.config.STRATEGY.VLM_ONLY.PROMPT_TYPE]
                return prompt
            prompt = prompt_strategy_map[self.config.STRATEGY.VLM_ONLY.PROMPT_TYPE]
        elif self.config.EXPERIMENT.TASK_NAME == "obj-centered-cls":
            prompt = prompt_strategy_map["task"]
        else:
            raise ValueError(f"Invalid task name: {self.config.EXPERIMENT.TASK_NAME}")
        
        return prompt

    ### Prompt for VLM-Only spatial reasoning ###
    def spatial_reasoning_prompt(self, **kwargs) -> str:
        metadata = kwargs.get("metadata", None)

        prompt = """Input:
        You are given a source image (the first image you see) and a target image (the second image you see). They are in the same scene but from source viewpoint and target viewpoint, respectively.

        Task:
        Suppose you are holding a camera, starting from source image viewpoint and moving to target image viewpoint. Your task is to determine the camera movement between the two images.
        """

        prompt_strategy_part = self._generate_prompt_strategy()

        options = total_options_map[self.config.EXPERIMENT.TASK_NAME][self.config.EXPERIMENT.TASK_SPLIT]["options"]
        options = self._postprocess_for_options(options)

        answer_candidates, option_map = _generate_options_text(options)

        promnpt_answer_part = """
        Answer Candidates:
        {answer_candidates}

        Response Format:
        Clearly explain your reasoning inside `<rsn></rsn>` tags, and then, provide your final decision inside `<ans></ans>` tags. 

        Response Format Example:
        <rsn>your reason here</rsn>
        <ans>just output the index of your answer</ans>
        """.format(
            answer_candidates=answer_candidates,
        )

        prompt = prompt + prompt_strategy_part + promnpt_answer_part
        
        return prompt, option_map

    ### Prompt for multi-agents reasoning ###
    def image_caption_prompt(self, **kwargs):
        metadata = kwargs.get("metadata", None)
        # prompt = """Input:
        # You are given a source image (the first image you see) and a target image (the second image you see). They are inthe  same scene but from source viewpoint and target viewpoint, respectively.

        # Task:
        # You are part of a multi-agent system. Your task is to understand the content of the images and provide a caption for each image. The caption should be concise and descriptive, capturing the key elements, the relative spatial relationships, and the occlusion of objects in the images. 

        # Response Format:
        # - Provide a caption for the source image inside `<src_caption></src_caption>` tags.
        # - Provide a caption for the target image inside `<tgt_caption></tgt_caption>` tags.
        # - Compare them and provide your comparison inside `<comparison></comparison>` tags.

        # Example Response Format:
        # <src_caption>Describe the source image here.</src_caption>
        # <tgt_caption>Describe the target image here.</tgt_caption>
        # <comparison>Compare the two images here.</comparison>
        # """
        optioins = total_options_map[self.config.TASK.NAME][self.config.TASK.SPLIT]["options"]

        is_trap = self.config.STRATEGY.IS_TRAP
        if is_trap:
            optioins.append("no movement")

        is_shuffle = self.config.STRATEGY.IS_SHUFFLE
        if is_shuffle:
            random.shuffle(optioins)

        answer_candidates, option_map = _generate_options_text(optioins)
        prompt = """Input:
        You are given a source image (the first image you see) and a target image (the second image you see). They are in the same scene but from source viewpoint and target viewpoint, respectively.

        Task:
        You are part of a multi-agent system. Your task is to understand the content of the images and infer the camera movement between the two images.

        Answer Candidates:
        {answer_candidates}

        Response Format:
        - If you are confident to make decision, clearly explain your reasoning inside `<rsn></rsn>` tags, and provide your final answer inside `<ans></ans>` tags. 

        Example Response Format:
        Able to judge the camera movement based on images:
        <rsn>My reason is...</rsn>
        <ans>the number of option you choose here</ans>
        """.format(
            answer_candidates=answer_candidates,
        )
        return prompt, option_map, answer_candidates

    def spatial_reasoning_prompt_ma(self, vlm_answer: str, **kwargs) -> str:
        metadata = kwargs.get("metadata", None)
        # optioins = total_options_map[self.config.TASK.NAME][self.config.TASK.SPLIT]["options"]

        # is_trap = self.config.STRATEGY.IS_TRAP
        # if is_trap:
        #     optioins.append("no movement")

        # is_shuffle = self.config.STRATEGY.IS_SHUFFLE
        # if is_shuffle:
        #     random.shuffle(optioins)

        # answer_candidates, option_map = _generate_options_text(optioins)

        option_map = kwargs.get('option_map', None)
        answer_candidates = kwargs.get('answer_candidates', None)

        # prompt = """Input: 
        # You are given a VLM's description on a source image and a target image, which is about the spatial relationship between the two images. The VLM's answer is as follows:
        # "{vlm_answer}"

        # Task:
        # You are part of a multi-agent system. Your task is to determine the camera movement based on the VLM's observations.

        # Answer Candidates:
        # {answer_candidates}

        # Response Format:
        # - If you are confident to make decision, clearly explain your reasoning inside `<rsn></rsn>` tags, and provide your final answer inside `<ans></ans>` tags. 
        # - If you need more information to make a decision, then ask your question to VLM inside `<ques></ques>` tags.

        # Example Response Format:
        # 1. Able to judge the camera movement based on the VLM's observations:
        # <rsn>My reason is...</rsn>
        # <ans>the option you choose here</ans>

        # 2. Need more information to make a decision:
        # <ques>Can you provide more details about...</ques>
        # """.format(
        #     vlm_answer=vlm_answer,
        #     answer_candidates=answer_candidates,
        # )

        prompt = """Input: 
        You are given a VLM's description on a source image and a target image, along with its justification on camera movement between image. The VLM's answer is as follows:
        "{vlm_answer}"

        Task:
        You are part of a multi-agent system. Your task is to determine if VLM's decision and its reasoning is reasonable.

        Answer Candidates:
        {answer_candidates}

        Response Format:
        - If you are confident to make decision, clearly explain your reasoning inside `<rsn></rsn>` tags, and provide your final answer inside `<ans></ans>` tags. 
        - If you need more information to make a decision, then ask your question to VLM inside `<ques></ques>` tags.

        Example Response Format:
        1. Able to judge the camera movement based on the VLM's observations:
        <rsn>My reason is...</rsn>
        <ans>the number of option you choose here</ans>

        2. Need more information to make a decision:
        <ques>Can you provide more details about...</ques>
        """.format(
            vlm_answer=vlm_answer,
            answer_candidates=answer_candidates,
        )
        return prompt, option_map